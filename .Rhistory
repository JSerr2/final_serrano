Avg_Chronic_Absenteeism = mean(Chronic_Absenteeism, na.rm = TRUE),
n = n()
)
View(summary_data)
# UI
ui <- fluidPage(
titlePanel("CEP Impact Explorer"),
sidebarLayout(
sidebarPanel(
selectInput(
"metric",
"Select Metric to Explore:",
choices = c(
"Average Attendance Rate" = "Avg_Attendance",
"Average Chronic Truancy Rate" = "Avg_Chronic_Truancy",
"Average Chronic Absenteeism Rate" = "Avg_Chronic_Absenteeism"
),
selected = "Avg_Attendance"
)
),
mainPanel(
plotOutput("impactPlot", height = "600px")
)
)
)
# Server
server <- function(input, output, session) {
output$impactPlot <- renderPlot({
metric <- input$metric
metric_label <- switch(
metric,
"Avg_Attendance" = "Average Attendance Rate",
"Avg_Chronic_Truancy" = "Average Chronic Truancy Rate",
"Avg_Chronic_Absenteeism" = "Average Chronic Absenteeism Rate"
)
# plot
ggplot(summary_data, aes(x = Poverty_Level, y = .data[[metric]], fill = Participates_in_CEP)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = paste(metric_label, "by Poverty Level and CEP Participation"),
x = "Poverty Level",
y = metric_label,
fill = "CEP Participation"
) +
theme_minimal() +
theme(
text = element_text(size = 14),
axis.text.x = element_text(angle = 45, hjust = 1)
)
})
}
shinyApp(ui, server)
View(county_acs)
View(shape_files_merged)
library(rvest)
library(tidyverse)
setwd("C:\\Users\\joses\\OneDrive\\Documents\\GitHub\\final_serrano")
# Function to scrape one page of search results
scrape_page <- function(url) {
page <- read_html(url)
# Extract titles
titles <- page %>% html_nodes(".Page-headline") %>% html_text(trim = TRUE)
# Extract dates
dates <- page %>% html_nodes(".Page-dateModified span") %>% html_text(trim = TRUE)
# Extract article links
links <- page %>%
html_nodes(".Link .Image , .PagePromo-content .PagePromoContentIcons-text") %>%
html_attr("href")
# Create a dataframe
data <- tibble(
Title = titles,
Date = dates,
Link = paste0("https://apnews.com", links) # Construct full URL
)
return(data)
}
# Function to scrape content from individual articles
scrape_article <- function(link) {
page <- read_html(link)
# Extract content paragraphs
content <- page %>% html_nodes("p") %>% html_text(trim = TRUE) %>% paste(collapse = " ")
return(content)
}
# Scrape multiple pages
base_url <- "https://apnews.com/search?q=%22Community%20Eligibility%20Provision%22%20OR%20%22school%20lunch%20programs%22%20&s=0&p="
all_data <- tibble()
# Loop through the first 5 pages of search results (adjust `max_pages` as needed)
max_pages <- 5
for (i in 1:max_pages) {
page_url <- paste0(base_url, i)
message("Scraping page: ", i)
# Scrape the page
page_data <- scrape_page(page_url)
# Append to the main dataset
all_data <- bind_rows(all_data, page_data)
}
# Scrape article content and save as text files
all_data <- all_data %>%
rowwise() %>%
mutate(Content = tryCatch(scrape_article(Link), error = function(e) NA))
# Save articles as individual text files
dir.create("Articles", showWarnings = FALSE)
for (i in 1:nrow(all_data)) {
file_name <- paste0("Articles/Article_", i, ".txt")
writeLines(c(all_data$Title[i], all_data$Date[i], all_data$Content[i]), file_name)
}
# Save a master dataset with titles, dates, and links
write_csv(all_data, "Articles/Article_Metadata.csv")
message("Scraping completed! Articles and metadata saved.")
library(rvest)
library(tidyverse)
setwd("C:\\Users\\joses\\OneDrive\\Documents\\GitHub\\final_serrano")
# Function to scrape one page of search results
scrape_page <- function(url) {
page <- read_html(url)
# Extract titles
titles <- page %>% html_nodes(".Page-headline") %>% html_text(trim = TRUE)
# Extract dates
dates <- page %>% html_nodes(".Page-dateModified span") %>% html_text(trim = TRUE)
# Extract article links
links <- page %>%
html_nodes(".Link .Image , .PagePromo-content .PagePromoContentIcons-text") %>%
html_attr("href")
# Create a dataframe
data <- tibble(
Title = titles,
Date = dates,
Link = paste0("https://apnews.com", links) # Construct full URL
)
return(data)
}
# Function to scrape content from individual articles
scrape_article <- function(link) {
page <- read_html(link)
# Extract content paragraphs
content <- page %>% html_nodes("p") %>% html_text(trim = TRUE) %>% paste(collapse = " ")
return(content)
}
# Scrape multiple pages
base_url <- "https://apnews.com/search?q=%22Community%20Eligibility%20Provision%22%20OR%20%22school%20lunch%20programs%22%20&s=0&p="
all_data <- tibble()
# Loop through the first 5 pages of search results (adjust `max_pages` as needed)
max_pages <- 5
for (i in 1:max_pages) {
page_url <- paste0(base_url, i)
message("Scraping page: ", i)
# Scrape the page
page_data <- scrape_page(page_url)
# Append to the main dataset
all_data <- bind_rows(all_data, page_data)
}
# Base URL for the search results
base_url <- "https://apnews.com/search?q=%22Community%20Eligibility%20Provision%22%20OR%20%22school%20lunch%20programs%22%20&s=0&p="
# Function to scrape article links from a search results page
scrape_links_from_page <- function(page_number) {
# Construct the URL for the current page
url <- paste0(base_url, page_number)
# Read the HTML of the page
page <- read_html(url)
# Extract article links
links <- page %>%
html_nodes(".PagePromo-content .PagePromo-title .PagePromoContentIcons-text") %>%
html_attr("href") %>%
str_subset("^/article/") # Keep only valid article links
# Create full URLs
full_links <- paste0("https://apnews.com", links)
return(full_links)
}
# Function to scrape an individual article
scrape_article <- function(link, article_number) {
article_page <- read_html(link)
# Extract title
title <- article_page %>%
html_node(".Page-headline") %>%
html_text(trim = TRUE)
# Extract article text
content <- article_page %>%
html_nodes("p") %>% # Selects all paragraph tags
html_text(trim = TRUE) %>%
paste(collapse = "\n") # Combines all paragraphs into a single string
# Write to a text file
file_name <- paste0("article_", article_number, ".txt")
writeLines(c(title, date, content), file_name)
}
# Scrape all pages
all_links <- unlist(lapply(1:5, scrape_links_from_page)) # Adjust 1:5 based on the number of pages
# Scrape and save all articles
for (i in seq_along(all_links)) {
tryCatch({
scrape_article(all_links[i], i)
Sys.sleep(1) # Add delay to avoid overloading the server
}, error = function(e) {
message(paste("Error scraping article", i, ":", e$message))
})
}
# Scrape all pages
all_links <- unlist(lapply(1:5, scrape_links_from_page)) # Adjust 1:5 based on the number of pages
all_links
# Scrape and save all articles
for (i in seq_along(all_links)) {
tryCatch({
scrape_article(all_links[i], i)
Sys.sleep(1) # Add delay to avoid overloading the server
}, error = function(e) {
message(paste("Error scraping article", i, ":", e$message))
})
}
library(rvest)
library(tidyverse)
# Function to scrape one page of search results
scrape_page <- function(url) {
page <- read_html(url)
# Extract article links
links <- page %>%
html_nodes("a.Link") %>%  # Selector for links
html_attr("href")
# Ensure full URLs
links <- paste0("https://apnews.com", links)
return(links)
}
# Function to scrape content from an individual article
scrape_article <- function(link) {
page <- read_html(link)
# Extract title
title <- page %>%
html_node(".Page-headline") %>%  # Selector for article title
html_text(trim = TRUE)
# Extract content paragraphs
content <- page %>%
html_nodes("p") %>%  # Selector for article content
html_text(trim = TRUE) %>%
paste(collapse = " ")  # Combine all paragraphs into one string
return(tibble(Title = title, Content = content, Link = link))
}
# Scrape multiple pages of search results
base_url <- "https://apnews.com/search?q=%22Community%20Eligibility%20Provision%22%20OR%20%22school%20lunch%20programs%22%20&s=0&p="
all_links <- c()
# Loop through pages 1 to 5
for (i in 1:5) {
page_url <- paste0(base_url, i)
message("Scraping page: ", i)
# Scrape links from the page
links <- scrape_page(page_url)
all_links <- c(all_links, links)  # Combine links from all pages
}
# Remove duplicate links
all_links <- unique(all_links)
all_links
# Scrape articles
articles <- tibble()
for (link in all_links) {
message("Scraping article: ", link)
# Try to scrape each article, skip on error
article <- tryCatch(scrape_article(link), error = function(e) NULL)
if (!is.null(article)) {
articles <- bind_rows(articles, article)
}
}
# Save articles to a CSV
write_csv(articles, "Articles/AP_News_Articles.csv")
# Save articles to a CSV
write_csv(articles, "Articles/AP_News_Articles.csv")
# Save individual articles as text files
dir.create("Articles", showWarnings = FALSE)
for (i in 1:nrow(articles)) {
file_name <- paste0("Articles/Article_", i, ".txt")
writeLines(c(articles$Title[i], articles$Content[i]), file_name)
}
message("Scraping completed! Articles and metadata saved.")
# Function to scrape one page of search results
scrape_page <- function(url) {
page <- read_html(url)
# Extract article links
links <- page %>%
html_nodes("a.Link") %>%
html_attr("href")
# Ensure links are full URLs
links <- ifelse(grepl("^https://", links), links, paste0("https://apnews.com", links))
return(links)
}
library(rvest)
library(tidyverse)
# Function to scrape one page of search results
scrape_page <- function(url) {
page <- read_html(url)
# Extract article links
links <- page %>%
html_nodes("a.Link") %>%
html_attr("href")
# Ensure links are full URLs
links <- ifelse(grepl("^https://", links), links, paste0("https://apnews.com", links))
return(links)
}
# Function to scrape content from an individual article
scrape_article <- function(link) {
page <- read_html(link)
# Extract title
title <- page %>%
html_node(".Page-headline") %>%  # Selector for article title
html_text(trim = TRUE)
# Extract content paragraphs
content <- page %>%
html_nodes("p") %>%  # Selector for article content
html_text(trim = TRUE) %>%
paste(collapse = " ")  # Combine all paragraphs into one string
return(tibble(Title = title, Content = content, Link = link))
}
# Scrape multiple pages of search results
base_url <- "https://apnews.com/search?q=%22Community%20Eligibility%20Provision%22%20OR%20%22school%20lunch%20programs%22%20&s=0&p="
all_links <- c()
# Loop through pages 1 to 5
for (i in 1:5) {
page_url <- paste0(base_url, i)
message("Scraping page: ", i)
# Scrape links from the page
links <- scrape_page(page_url)
all_links <- c(all_links, links)  # Combine links from all pages
}
# Remove duplicate links
all_links <- unique(all_links)
all_links
# Scrape articles
articles <- tibble()
for (link in all_links) {
message("Scraping article: ", link)
# Try to scrape each article, skip on error
article <- tryCatch(scrape_article(link), error = function(e) NULL)
if (!is.null(article)) {
articles <- bind_rows(articles, article)
}
}
View(articles)
# Save articles to a CSV
write_csv(articles, "Articles/AP_News_Articles.csv")
View(articles)
View(articles)
library(tidytext)
tidy_article <- articles %>%
unnest_tokens(word, content) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$"))
View(articles)
tidy_article <- articles %>%
unnest_tokens(word, Content) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$"))
View(tidy_article)
# Word Frequency Analysis
word_freq <- tidy_articles %>%
count(word, sort = TRUE)
# Word Frequency Analysis
word_freq <- tidy_article %>%
count(word, sort = TRUE)
View(word_freq)
View(articles)
View(articles)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"Copyright 2024 The Associated Press. All Rights Reserved.")
)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"Copyright 2024 The Associated Press. All Rights Reserved.", "")
)
tidy_article <- articles %>%
unnest_tokens(word, Content) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$"))
View(tidy_article)
word_freq <- tidy_articles %>%
count(word, sort = TRUE)
word_freq <- tidy_article %>%
count(word, sort = TRUE)
View(word_freq)
View(articles)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"\\bAP\\b", "")
)
View(articles)
tidy_article <- articles %>%
unnest_tokens(word, Content) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$"))
word_freq <- tidy_article %>%
count(word, sort = TRUE)
View(word_freq)
word_freq %>%
slice_max(n, n = 15) %>%
ggplot(aes(reorder(word, n), n)) +
geom_bar(stat = "identity", fill = "blue") +
coord_flip() +
labs(title = "Most Common Words in Articles",
x = "Words",
y = "Frequency")
View(articles)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"photo", "")
)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"FILE", "")
)
tidy_article <- articles %>%
unnest_tokens(word, Content) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$"))
word_freq <- tidy_article %>%
count(word, sort = TRUE)
word_freq %>%
slice_max(n, n = 15) %>%
ggplot(aes(reorder(word, n), n)) +
geom_bar(stat = "identity", fill = "blue") +
coord_flip() +
labs(title = "Most Common Words in Articles",
x = "Words",
y = "Frequency")
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"Copyright 2024 The Associated Press. All Rights Reserved.", "")
)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"\\bAP\\b", "")
)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"photo", "")
)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"FILE", "")
)
tidy_article <- articles %>%
unnest_tokens(word, Content) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$"))
word_freq <- tidy_article %>%
count(word, sort = TRUE)
View(word_freq)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"Photo", "")
)
articles <- articles %>%
mutate(
Content = str_replace_all(Content,
"FILE", "")
)
tidy_article <- articles %>%
unnest_tokens(word, Content) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$"))
word_freq <- tidy_article %>%
count(word, sort = TRUE)
word_freq %>%
slice_max(n, n = 15) %>%
ggplot(aes(reorder(word, n), n)) +
geom_bar(stat = "identity", fill = "blue") +
coord_flip() +
labs(title = "Most Common Words in Articles",
x = "Words",
y = "Frequency")
View(word_freq)
text_plot1 <- word_freq %>%
slice_max(n, n = 15) %>%
ggplot(aes(reorder(word, n), n)) +
geom_bar(stat = "identity", fill = "blue") +
coord_flip() +
labs(title = "Most Common Words in Articles",
x = "Words",
y = "Frequency")
ggsave("C:\\Users\\joses\\OneDrive\\Documents\\GitHub\\final_serrano\\Images\\plot1_text_analys", text_plot1)
ggsave("C:\\Users\\joses\\OneDrive\\Documents\\GitHub\\final_serrano\\Images\\plot1_text_analys.png", text_plot1)
sentiment_analysis <- tidy_article %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(sentiment, sort = TRUE)
View(sentiment_analysis)
sentiment_analysis %>%
ggplot(aes(x = sentiment, y = n, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis of Articles",
x = "Sentiment",
y = "Count")
text_plot2 <- sentiment_analysis %>%
ggplot(aes(x = sentiment, y = n, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis of Articles",
x = "Sentiment",
y = "Count")
ggsave("C:\\Users\\joses\\OneDrive\\Documents\\GitHub\\final_serrano\\Images\\plot2_text_analys.png", text_plot2)
model_absenteeism <- lm(
Chronic_Absenteeism ~ Participates_in_CEP * Poverty_Level + `%_Student_Enrollment_-_Low_Income` + Title_1_Binary,
data = merged_data
)
summary(model_absenteeism)
model_truancy <- lm(
Student_Chronic_Truancy_Rate ~ Participates_in_CEP * Poverty_Level + `%_Student_Enrollment_-_Low_Income` + Title_1_Binary,
data = merged_data
)
summary(model_truancy)
